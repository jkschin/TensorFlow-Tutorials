<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Neural Network Adder</title>
  <meta name="description" content="TensorFlow Tutorials rehashed for beginners.">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/tutorials/neural-network-adder/">
  <link rel="alternate" type="application/rss+xml" title="TensorFlow Tutorials" href="/feed.xml">
  
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">

    <a class="site-title" href="/">TensorFlow Tutorials</a>

    <nav class="site-nav">
      <span class="menu-icon">
        <svg viewBox="0 0 18 15" width="18px" height="15px">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </span>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>
</header>

    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <div class="post-content">
    <h1 id="neural-network-adder"><strong>Neural Network Adder</strong></h1>

<ol>
  <li><a href="#inference">Inference</a></li>
  <li><a href="#loss">Loss</a></li>
  <li><a href="#train">Train</a></li>
  <li><a href="#train-loop">Train Loop</a></li>
  <li><a href="#printing-values">Printing Values</a></li>
  <li><a href="#name-scopes-and-variable-scopes">Name Scopes and Variable Scopes</a></li>
  <li><a href="#saving-a-model">Saving a Model</a></li>
  <li><a href="#loading-a-model">Loading a Model</a></li>
</ol>

<p>If you’re reading this, you’ve probably decided to pick up deep learning. We start off with a simple TensorFlow implementation of a neural network adder. We do this for 2 reasons:</p>

<ol>
  <li>It’s really simple! It trains really fast.</li>
  <li>You don’t have to download other data sets like MNIST or CIFAR, or something else.</li>
</ol>

<p>You can run it from your laptop within 10 seconds and see the results. Without further ado, let’s begin.</p>

<h1 id="inference"><strong>Inference</strong></h1>

<p>We start with inference (diagram to be inserted).</p>
<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">inference</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
  <span class="n">weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'weights'</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">)</span>
  <span class="n">biases</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'biases'</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">)</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="n">biases</span>
  <span class="k">return</span> <span class="n">result</span>
</code></pre>
</div>

<p>That’s pretty much it. You’re done for inference.</p>

<h1 id="loss"><strong>Loss</strong></h1>

<p>We use simple L2 loss for this. It makes sense because:</p>

<ol>
  <li>If the neural network outputs a 0, but the answer is 1, the loss will be 0.5.</li>
  <li>If the neural network outputs a 0, but the answer is 0, the loss will be 0.</li>
  <li>If the neural network outputs a 1, but the answer is 1, the loss will be 0.</li>
  <li>And so on…</li>
</ol>

<p>You can check out <a href="https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html">this link</a> for more options.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># result: output of neural network</span>
<span class="c"># gt: ground truth</span>
<span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">gt</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_loss</span><span class="p">(</span><span class="n">result</span> <span class="o">-</span> <span class="n">gt</span><span class="p">)</span>
</code></pre>
</div>

<h1 id="train"><strong>Train</strong></h1>

<p>With the loss function defined, we need some kind of optimizer. We will use Adam Optimizer for this because it is pretty simple to use. You can check out <a href="https://www.tensorflow.org/versions/r0.11/api_docs/python/train.html#optimizers">this link</a> for more options.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">global_step</span><span class="p">):</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">()</span>
  <span class="n">train_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">global_step</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">train_op</span>
</code></pre>
</div>

<h1 id="train-loop"><strong>Train Loop</strong></h1>

<p>With that, you are ready to write the train loop to feed in the data, get the loss, and tune the weights!</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_loop</span><span class="p">():</span>
  <span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'global_step'</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
  <span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'data'</span><span class="p">)</span>
  <span class="n">gt</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s">'gt'</span><span class="p">)</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">inference</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">gt</span><span class="p">)</span>
  <span class="n">train_op</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">global_step</span><span class="p">)</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">initialize_all_variables</span><span class="p">())</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10001</span><span class="p">):</span>
      <span class="n">data_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">gt_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">pair</span><span class="p">)</span> <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">data_in</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">_</span><span class="p">,</span> <span class="n">loss_val</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">train_op</span><span class="p">,</span> <span class="n">loss</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">data</span><span class="p">:</span> <span class="n">data_in</span><span class="p">,</span> <span class="n">gt</span><span class="p">:</span> <span class="n">gt_in</span><span class="p">})</span>
      <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span> <span class="s">"Step: </span><span class="si">%</span><span class="s">d, Loss: </span><span class="si">%</span><span class="s">f"</span> <span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">loss_val</span><span class="p">)</span>
</code></pre>
</div>

<p>As you can see, by Step 10000, the loss has approached 0.000000. While this is a good result (unless of course you code your loss function wrongly), you can’t see the values of the weights and biases changing with each step. You can’t see sample inputs and corresponding results as well!</p>

<h1 id="printing-values"><strong>Printing Values</strong></h1>

<p>So now you’re thinking of printing the values of the weights and biases at every iteration, and see how they change. Perhaps you might even want to see the random data generated in each iteration and the result that the neural network gives. If you’re impatient, you can skim through the first few parts, but I thought it would be useful to talk about this as most people would go through this. Coming from a Python background, most people might now do something like this:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10001</span><span class="p">):</span>
  <span class="o">...</span>
  <span class="o">...</span>
  <span class="k">print</span> <span class="p">(</span><span class="n">weights</span><span class="p">)</span>
  <span class="k">print</span> <span class="p">(</span><span class="n">biases</span><span class="p">)</span>
  <span class="k">print</span> <span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre>
</div>

<p>You then realize that this doesn’t actually work because these variables are not local to the function train_loop. You then decide to return these variables:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">inference</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
  <span class="o">...</span>
  <span class="o">...</span>
  <span class="k">return</span> <span class="n">result</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span>

<span class="k">def</span> <span class="nf">train_loop</span><span class="p">():</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10001</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="o">...</span>
    <span class="k">print</span> <span class="p">(</span><span class="n">weights</span><span class="p">)</span>
    <span class="k">print</span> <span class="p">(</span><span class="n">biases</span><span class="p">)</span>
    <span class="k">print</span> <span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre>
</div>

<p>You do get some results! However, they look weird and you have no idea what it is.</p>

<p>Tensor(“add:0”, shape=(?, 1), dtype=float32)</p>

<p>Well, that’s because this variable is actually something like a node in the graph and you have to run it to get the values. Specifically, this is an add operation that outputs a size of (?, 1), where ? is the batch_size inferred, and the dtype is float32. You then call sess.run() like so:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">_</span><span class="p">,</span> <span class="n">loss_val</span><span class="p">,</span> <span class="n">weights_val</span><span class="p">,</span> <span class="n">biases_val</span><span class="p">,</span> <span class="n">result_val</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">train_op</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">result</span><span class="p">])</span>
<span class="k">print</span> <span class="n">loss_val</span>
<span class="k">print</span> <span class="n">weights_val</span>
<span class="k">print</span> <span class="n">biases_val</span>
<span class="k">print</span> <span class="n">result_val</span>
</code></pre>
</div>

<p>Voila! It works! But this is a really ugly way to do it. I had to walk you through this because you will see the beauty of tf.add_to_collection() only after this.</p>

<p>You should actually modify inference and train_loop to the following:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">inference</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
  <span class="n">weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'weights'</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">)</span>
  <span class="n">biases</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'biases'</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">)</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="n">biases</span>

  <span class="c"># You can see tf.add_to_collection as adding these graph nodes for easy access later.</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="s">'weights'</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="s">'biases'</span><span class="p">,</span> <span class="n">biases</span><span class="p">)</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="s">'result'</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">result</span>

<span class="k">def</span> <span class="nf">train_loop</span><span class="p">():</span>
  <span class="o">...</span>
  <span class="o">...</span>
  <span class="k">print</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="s">'weights'</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">name</span>
  <span class="k">print</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="s">'weights'</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span> <span class="c"># Notice the weights value changing.</span>
  <span class="k">print</span> <span class="s">""</span>
  <span class="k">print</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="s">'biases'</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">name</span>
  <span class="k">print</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="s">'biases'</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span> <span class="c"># Notice the bias value changing.</span>
  <span class="k">print</span> <span class="s">""</span>
  <span class="k">print</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="s">'result'</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">name</span>
  <span class="k">print</span> <span class="n">data_in</span>
  <span class="k">print</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="s">'result'</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">data</span><span class="p">:</span> <span class="n">data_in</span><span class="p">})</span> <span class="c"># Notice feed_dict is required here because result depends on data.</span>
  <span class="k">print</span> <span class="s">""</span>
</code></pre>
</div>

<p>You can now see the weights and biases change and be sure that the neural network adder is doing what it is supposed to do. You can also print the names of these nodes as well. While this is really nice, when you have very complex graphs, you might not want to call your variables ‘weights1’, ‘weights2’, and so on. That’s what name scopes and variable scopes are for.</p>

<h1 id="name-scopes-and-variable-scopes"><strong>Name Scopes and Variable Scopes</strong></h1>

<p>This is a really short section. Name scopes are variable scopes are extremely useful because it really makes everything much cleaner. All you have to do is to add 2 lines and you’re done!</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">inference</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'inference'</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">'hidden1'</span><span class="p">):</span>
      <span class="n">weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'weights'</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">)</span>
      <span class="n">biases</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'biases'</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">)</span>
      <span class="n">result</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="n">biases</span>

  <span class="c"># You can see tf.add_to_collection as adding these graph nodes for easy access later.</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="s">'weights'</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="s">'biases'</span><span class="p">,</span> <span class="n">biases</span><span class="p">)</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="s">'result'</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">result</span>
</code></pre>
</div>

<p>There are important things to note, however. You can see that the names of your nodes are now the following:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">hidden1</span><span class="o">/</span><span class="n">weights</span><span class="p">:</span><span class="mi">0</span>
<span class="n">hidden1</span><span class="o">/</span><span class="n">biases</span><span class="p">:</span><span class="mi">0</span>
<span class="n">inference</span><span class="o">/</span><span class="n">hidden1</span><span class="o">/</span><span class="n">add</span><span class="p">:</span><span class="mi">0</span>
</code></pre>
</div>

<p>Wait a minute, isn’t that weird? Shouldn’t the weights and biases have inference appended to it as well? In short, name_scopes are not appended to variables. But the add op has both name_scope and variable_scope appended. Do read the TensorFlow documentation for a greater elaboration.</p>

<h1 id="saving-a-model"><strong>Saving a Model</strong></h1>

<p>The first thing you have to do is to create a saver. After creating a saver, simply call it when you want the model to be saved. You can do it like this:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_loop</span><span class="p">():</span>
  <span class="o">...</span>
  <span class="o">...</span>
  <span class="n">train_op</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">global_step</span><span class="p">)</span>
  <span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">initialize_all_variables</span><span class="p">())</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10001</span><span class="p">):</span>
      <span class="o">...</span>
      <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="o">...</span>
        <span class="o">...</span>
        <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s">'train_dir/my-model'</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
</code></pre>
</div>

<p>And you’re done! The results will be written to train_dir. There are three interesting things in train_dir</p>

<ol>
  <li>checkpoint. This is a record of all the latest checkpoints.</li>
  <li>my-model-XXXX. These are the weights at that point in time.</li>
  <li>my-model-XXXX.meta. This is the entire graph definition that you can load in future for inference or re-training.</li>
</ol>

<h1 id="loading-a-model"><strong>Loading a Model</strong></h1>

<p>To be continued…</p>

  </div>

</article>
      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">TensorFlow Tutorials</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              TensorFlow Tutorials
            
            </li>
            
            <li><a href="mailto:samuelchin91@gmail.com">samuelchin91@gmail.com</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/jkschin"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">jkschin</span></a>

          </li>
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>TensorFlow Tutorials rehashed for beginners.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
